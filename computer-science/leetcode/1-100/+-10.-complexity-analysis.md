# + 10. Complexity Analysis

## Considering the Complexity <a id="7472"></a>

The time complexity for this question appears to be difficult to pin down. After all there’s two points of recursion and one full stop. And like I said before, the answer to the complexity is given in the solutions with no further explanations. For text of length T and pattern of length P, with the text being indexed at text\[i:\] and pattern\[2j:\], the time complexity for this question is….\***drum roll\***

![](https://miro.medium.com/max/670/1*WDxQvjHdkIDbiMeyeNW-Vg.png)

Well that’s quite something.

Normally you’d expect something like O\(T\*P\) or anything else to that effect and in fact the solutions do propose a more concise upper bound, again without any explanations \(not surprisingly\), but we’ll get to that later. For now we have enough on our hands as it is with this piece of work.

## Linear vs Exponential Complexity <a id="5eff"></a>

The reason why the complexity isn’t as simple as it is for something like [bubble sort](https://en.wikipedia.org/wiki/Bubble_sort), is because we’re not making linear passes over our text and pattern. Instead we’re doing things recursively, diving deeper and deeper through sections of our text and pattern and revisiting the same sub problems for what can feel like an arbitrary number of times. The reason I bring up bubble sort, which has O\(n²\) complexity, is because its complexity decreases in the shape of a pyramid as we sort more and more digits.

![](https://miro.medium.com/max/1266/1*JfJ-i1r5dtVIU8Tj2vsUQA.png)

On the other hand recursion has the classic shape of a tree due to the multiple places where the function can call itself.

![](https://miro.medium.com/max/1686/1*8SKs4LuB5mozG4kWY362Fg.png)

The structures look deceptively similar but the crucial difference is that where the pyramid decreases linearly, the tree grows exponentially. This creates a world of a difference when analyzing time and space complexity.

## Our Worst Case <a id="407b"></a>

When analyzing worst case complexity we have to think of the worst case input. Because we have two inputs, text and pattern, I will focus on the pattern because one depends on the other and at least for me it’s easier to think of the rules rather than the results. In this question each new character we add to our pattern can be either a `.*` or `.` where `.` can be any character. From the code it should be pretty clear which choice causes more computation. If we include the `*` character, our code splits into two streams which must both be computed until inevitable they should return false, meanwhile if we are matching a single character the fun stops pretty quickly as we are only taking a single route, linearly.

![](https://miro.medium.com/max/1228/1*GCo1eZSZOuAU1CrKJmvB6Q.png)

It looks pretty clear that a worst case pattern would consist of many `.*` to create as many diverging paths as possible, with a final text character which does not match the pattern.

**Text: aaaaaaaaaab**

**Pattern: a\*a\*a\*a\*a\*a\*a\*a\*a\*a**

In this scenario we must get both to the end of the pattern and to the end of the string to determine that there is no match, while branching 2¹⁰ number of times just from the first index of the text.

## Visualized the Computation <a id="ebfa"></a>

In order to figure out the time complexity we have to find how many times each sub-problem had to be computed. Because we are not saving the result like in dynamic programming this will largely contribute to the exploding runtime. Each sub problem consists of answering whether text\[i:\] and pattern\[2j:\] are a match. We use pattern\[2j:\] because we are using `.*`repeatedly in our pattern for the worst case, every time we take the **skip\_asterisk\_matched** path we skip one of the `.*` which take up two spaces. In order to visualize the two branches of **skip\_asterisk\_matched** and **keep\_asterisk\_matched** we will draw a tree diagram to illustrate all the visited sub problems. Each node will have the format \(i,j\) for where the text and pattern are indexed at each point in time.

![](https://miro.medium.com/max/1542/1*sivOYt3w3bFykB5I7g18Rw.png)

A quick side note on this, when performing recursion, the algorithm will perform a [depth first search](https://en.wikipedia.org/wiki/Depth-first_search) by taking the left path first on each iteration. This doesn’t make much of a difference though because in the worst case we’ll have to traverse the entire tree anyways. Let’s take an example sub-problem where we want to see if text\[2:\] and pattern\[2:\] were a match. This will appear as \(2,1\) on our tree.

![](https://miro.medium.com/max/1688/1*Wb-O_dvBufcSSnJiaLkbIQ.png)

We can see that this sub-problem had been encountered 3 times during computation. The tree format is good for seeing when these sub-problems will be encountered but it’s not very good at determining how many times that will happen.

## Combinations <a id="fea8"></a>

The way to systematically consider how many times a sub-problem will be encountered is to consider how come these sub problems tend to repeat themselves in the first place. In order to arrive at \(2,1\) on our tree we have to take two lefts and one right.

![](https://miro.medium.com/max/1686/1*68esAT8bjb2XiX7ckY3F9Q.png)

As we can see this can happen in three ways, if we traverse the tree in the sequence LLR, LRL and RLL.

![](https://miro.medium.com/max/1008/1*--kEbYPRvCkIoxbAGTAJew.png)

In the example of \(2,1\) we know that we will have 2+1=3 place holders where 2 of the placeholders will be an L and 1 will be an R. In order to find how many [combinations](https://en.wikipedia.org/wiki/Combination) are possible we can simply compute 3C2 where out of 3 place holders we will choose 2 to be an L and the rest will automatically be Rs.

That means that for sub-problem \(i,j\) we have i+j placeholders with i of those being the left side and j being on the right side. This is why the solutions include the combinations symbol for the number of encountered sub-problems \(i,j\) with \(i+j choose i\).

![](https://miro.medium.com/max/190/1*IWzQCnsA-5hbauRv-_EEag.png)

Incidentally this is also equivalent to \(i+j choose j\) because it does not matter whether you are picking which i increments are on the left or instead picking which j increments are on the right, the number of distinct combinations will be the same.

![](https://miro.medium.com/max/182/1*JMQCWjrhiLVWljIaS1eLiw.png)

## Sub-Problem Complexity <a id="0b4f"></a>

Now that we know how many sub-problems of each kind we will encounter, we should figure out how long those individual sub-problem will take to complete. This sort of thing tends to be very easy to do with recursive problems, like in this case where we only really do one thing, compare a single character and store the result in **character\_match**. This might lead you to believe that the runtime for each sub-problem is constant O\(1\), but in order to call the function we must first supply text\[i\] and pattern\[j:\] arguments. If the strings are long, this procedure is not trivial, especially in python where in order to pass in a sub-string a new string must be created. Therefore for each sub-problem there is a constant time O\(1\) for checking that the character matched, an O\(T-i\) for passing in the part of the string that we don’t know is a match yet, and an O\(P-2j\) for passing in the part of the pattern that we don’t know is a match yet. Adding everything together we arrive at the complexity given in the solution.

![](https://miro.medium.com/max/670/1*WDxQvjHdkIDbiMeyeNW-Vg.png)

## **The Concise Solution** <a id="6f57"></a>

Remember earlier when I said that the solution proposed a more general bound, without the complex combination notation and sigma signs.

Well here it is.

![](https://miro.medium.com/max/536/1*NcpJHep2qSL-SLyC9xI7Og.png)

Let’s go through the idea behind this one before we wrap up.

The idea behind making the time complexity into this simplified form is to go overkill on both how many sub problems we could encounter and how long we plan on those sub problems taking to compute.

**Number of Sub-problems**

Instead of using combinations to compute precisely how many sub problems we will encounter, which by [Pascal’s rule](https://en.wikipedia.org/wiki/Pascal%27s_rule) will be only a portion of a tree’s leaf nodes, let’s just take the entire bottom level of the tree. And let’s also make the tree completely full with a depth of the maximum number that i+j can take, \(T+P/2\). This will give us 2^\(T+P/2\) nodes.

![](https://miro.medium.com/max/1800/0*Z4rBE0IlADAKFK_O.jpg)

In the image we can see Pascal’s rule. If before we were taking one of the cells as our answer for determining the number of sub-problems, to simplify things we will take the sum of the entire bottom most row, just to be on the safe side.

**Sub-problem Complexity**

Instead of computing how large the arguments will be before we pass them in, let’s just assume we are passing in the entire strings for both the text and pattern. This will give us O\(T+P\) time complexity for each sub-problem.

**Final Answer**

Multiplying the Sub-problem complexity by the number of sub-problems gives us the final answer of:

![](https://miro.medium.com/max/536/1*NcpJHep2qSL-SLyC9xI7Og.png)

